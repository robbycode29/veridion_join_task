# -*- coding: utf-8 -*-
"""veridion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12CAYuGmBor40NeYKSn56jQvtOoPiYXfV

## First look
"""

# !unzip datasets.zip

from IPython import get_ipython
from IPython.display import display
import pandas as pd

facebook_df = pd.read_csv('facebook_dataset.csv', on_bad_lines='skip')
google_df = pd.read_csv('google_dataset.csv', on_bad_lines='skip')
website_df = pd.read_csv('website_dataset.csv', on_bad_lines='skip', delimiter=';')

# Inspect Facebook dataset
print(facebook_df.info())
print(facebook_df.head())

# Inspect Google dataset
print(google_df.info())
print(google_df.head())

# Inspect Website dataset
print(website_df.info())
print(website_df.head())

# Get columns of each dataset
facebook_columns = set(facebook_df.columns)
google_columns = set(google_df.columns)
website_columns = set(website_df.columns)

# Find common columns
common_columns = facebook_columns & google_columns & website_columns
print("Common columns:", common_columns)

# Check for missing values
print(facebook_df.isnull().sum(), '\n\n')
print(google_df.isnull().sum(), '\n\n')
print(website_df.isnull().sum(), '\n\n')

# Check for duplicates
print('facebook: ', facebook_df.duplicated().sum())
print('google: ', google_df.duplicated().sum())
print('website: ', website_df.duplicated().sum())

"""## Pre-processing and Clean up"""

# Standardizing website columns

website_df.rename(
    columns={
        'root_domain': 'domain',
        'main_city': 'city',
        'main_country': 'country',
        'main_region': 'region',
        'site_name': 'name',
        's_category': 'category',
        },
    inplace=True)

# Standardizing facebook columns

facebook_df.rename(
    columns={
        'country_name': 'country',
        'region_name': 'region',
        },
    inplace=True)

# Standardizing google columns

google_df.rename(
    columns={
        'country_name': 'country',
        'region_name': 'region',
        'text': 'description'
        },
    inplace=True)

# Find common columns after renaming
facebook_columns = set(facebook_df.columns)
google_columns = set(google_df.columns)
website_columns = set(website_df.columns)

common_columns_after_rename = facebook_columns & google_columns & website_columns
print("Common columns after renaming:", common_columns_after_rename)

# Split the 'Category' column by '|' and create a list
facebook_df['category'] = facebook_df['categories'].str.split('|')
facebook_df.drop('categories', axis=1, inplace=True)

print(facebook_df.head())

def to_list(value):
    if isinstance(value, list):
        return value
    elif pd.isna(value):
        return []
    else:
        return [value]

facebook_df['category'] = facebook_df['category'].apply(to_list)
google_df['category'] = google_df['category'].apply(to_list)
website_df['category'] = website_df['category'].apply(to_list)

# Convert 'phone' column to object type for all dataframes
facebook_df['phone'] = facebook_df['phone'].astype(str)
google_df['phone'] = google_df['phone'].astype(str)
website_df['phone'] = website_df['phone'].astype(str)

# Company Names
# Convert to lowercase to ensure case-insensitive comparisons.
# Trim leading/trailing spaces.
# Remove special characters (&, ,, ., /, - inconsistencies).
# Normalize common abbreviations (e.g., Inc. → Incorporated, Ltd. → Limited).

import re

def standardize_company_name(name):
    """
    Standardizes company names for consistent comparisons.
    """
    if not isinstance(name, str):  # Handle cases where name might not be a string
        return ""

    name = name.lower()
    name = name.strip()
    name = re.sub(r'[&.,/\\-]', '', name)  # Remove special characters

    # Normalize common abbreviations
    name = name.replace('inc', 'incorporated')
    name = name.replace('ltd', 'limited')
    name = name.replace('llc', 'limited liability company')
    name = name.replace('co', 'company')
    name = name.replace('corp', 'corporation')

    # account for foreign alphabets
    name = re.sub(r'[^\x00-\x7F]+', '', name)
    name = re.sub(r'[^\w\s]', '', name)
    name = re.sub(r'\s+', ' ', name)

    return name

# Example usage (assuming you have a 'Company Name' column in your DataFrame)
facebook_df['name'] = facebook_df['name'].apply(standardize_company_name)
google_df['name'] = google_df['name'].apply(standardize_company_name)
website_df['name'] = website_df['name'].apply(standardize_company_name)

print(facebook_df.head())
print(google_df.head())
print(website_df.head())

# Phone Numbers
# Remove non-numeric characters (+, -, (), spaces).
# Ensure consistent formats (e.g., international format: +1-xxx-xxx-xxxx).
# Check for invalid phone numbers (e.g., too short/long).

def standardize_phone_number(phone):
    """
    Standardizes phone numbers to the international format: +1-xxx-xxx-xxxx.
    """
    if not isinstance(phone, str):
        return ""  # Handle non-string values

    phone = re.sub(r'[^\d+]', '', phone) # Remove non-numeric characters except for '+'

    if not phone.startswith('+') and len(phone) != 0:
        phone = '+' + phone

    return phone

facebook_df['phone'] = facebook_df['phone'].apply(standardize_phone_number)
google_df['phone'] = google_df['phone'].apply(standardize_phone_number)
website_df['phone'] = website_df['phone'].apply(standardize_phone_number)

print(facebook_df.head())
print(google_df.head())
print(website_df.head())

def standardize_domain(domain):
    """
    Standardizes domain names and checks for common TLD formats.
    """
    if not isinstance(domain, str):
        return None

    domain = domain.lower().strip()
    domain = re.sub(r'^www\.', '', domain)  # Remove leading www.

    # Basic TLD check (can be expanded for more TLDs)
    common_tlds = ['.com', '.org', '.net', '.edu', '.gov', '.co.uk', '.co.in', '.ca']
    for tld in common_tlds:
        if domain.endswith(tld):
            return str(domain)

    # Attempt to extract a TLD if not found above
    match = re.search(r'\.[a-z]{2,6}$', domain)  # Match . followed by 2-6 letters at the end
    if match:
        return str(domain)


website_df['domain'] = website_df['domain'].apply(standardize_domain)
facebook_df['domain'] = facebook_df['domain'].apply(standardize_domain)
google_df['domain'] = google_df['domain'].apply(standardize_domain)
print(website_df.head())
print(facebook_df.head())
print(google_df.head())

"""# Analytics

### Data Uniqueness Analysis
"""

def analyze_uniqueness(df, column_name):
    """
    Analyzes the uniqueness of values in a specified column of a DataFrame.

    Args:
        df: The input DataFrame.
        column_name: The name of the column to analyze.

    Returns:
        A dictionary containing the percentage of unique values and the total number of unique values.
    """
    total_values = len(df)
    unique_values = df[column_name].nunique()
    percentage_unique = round((unique_values / total_values) * 100, 2) if total_values > 0 else 0
    return {"percentage_unique": percentage_unique, "unique_count": unique_values}


columns_to_analyze = ['domain', 'name', 'phone']
results = {}

for col in columns_to_analyze:
  results[f'facebook_{col}'] = analyze_uniqueness(facebook_df, col)
  results[f'google_{col}'] = analyze_uniqueness(google_df, col)
  results[f'website_{col}'] = analyze_uniqueness(website_df, col)

# Print the results
for key, value in results.items():
    print(f"{key}: {value}")

import matplotlib.pyplot as plt
import numpy as np

# Data for the bar chart
categories = ['facebook_domain', 'google_domain', 'website_domain',
              'facebook_name', 'google_name', 'website_name',
              'facebook_phone', 'google_phone', 'website_phone']
percentage_unique = [results['facebook_domain']['percentage_unique'],
                     results['google_domain']['percentage_unique'],
                     results['website_domain']['percentage_unique'],
                     results['facebook_name']['percentage_unique'],
                     results['google_name']['percentage_unique'],
                     results['website_name']['percentage_unique'],
                     results['facebook_phone']['percentage_unique'],
                     results['google_phone']['percentage_unique'],
                     results['website_phone']['percentage_unique']]

key_column_colors = {
    'domain': 'blue',
    'name': 'orange',
    'phone': 'purple'
}
# Create a color map that groups colors by the key column.
bar_colors = []
for cat in categories:
  if 'domain' in cat:
    bar_colors.append(key_column_colors['domain'])
  elif 'name' in cat:
    bar_colors.append(key_column_colors['name'])
  else:
    bar_colors.append(key_column_colors['phone'])

# Create the bar chart
plt.figure(figsize=(14, 8))  # Adjust figure size as needed

plt.bar(categories, percentage_unique, color=bar_colors)
plt.xlabel("Data Source and Column", fontsize=12)
plt.ylabel("Percentage of Unique Values", fontsize=12)
plt.title("Uniqueness Analysis of Key Columns", fontsize=14)
plt.xticks(rotation=45, ha='right', fontsize=10)  # Rotate x-axis labels for better readability
plt.tight_layout()  # Adjust layout to prevent labels from overlapping

# Create custom legend for key columns
key_patches = [plt.Rectangle((0, 0), 1, 1, color=key_column_colors[key]) for key in key_column_colors]
plt.legend(key_patches, key_column_colors.keys(), title="Key Columns", loc='upper right')

plt.show()

"""### Distribution & Coverage Analysis"""

# Calculate the percentage of missing values for each column in each DataFrame.
facebook_missing_percentage = facebook_df.isnull().sum() / len(facebook_df) * 100
google_missing_percentage = google_df.isnull().sum() / len(google_df) * 100
website_missing_percentage = website_df.isnull().sum() / len(website_df) * 100

# Identify the dataset with the most missing values per column.
print("Facebook Missing Value Percentage:\n", facebook_missing_percentage)
print("\nGoogle Missing Value Percentage:\n", google_missing_percentage)
print("\nWebsite Missing Value Percentage:\n", website_missing_percentage)

# Find the dataset with the maximum missing values for each column.
max_missing_df = pd.DataFrame({
    'Facebook': facebook_missing_percentage,
    'Google': google_missing_percentage,
    'Website': website_missing_percentage
}).idxmax(axis=1)
print("\nDataset with Maximum Missing Values per Column:\n", max_missing_df)

# Analyze for systematic missingness (e.g., websites missing phone numbers)
print("\nSystematic Missingness:")
print("Website Phone Number Missing Percentage:", website_missing_percentage['phone'])
print("Google Phone Number Missing Percentage:", google_missing_percentage['phone'])
print("Facebook Phone Number Missing Percentage:", facebook_missing_percentage['phone'])

key_columns = common_columns_after_rename

# Create a dictionary to store the missing percentages for each dataset and column.
missing_data = {}
for col in key_columns:
    missing_data[f'facebook_{col}'] = facebook_missing_percentage[col] if col in facebook_missing_percentage.index else 0
    missing_data[f'google_{col}'] = google_missing_percentage[col] if col in google_missing_percentage.index else 0
    missing_data[f'website_{col}'] = website_missing_percentage[col] if col in website_missing_percentage.index else 0

# Prepare the data for plotting.
categories = list(missing_data.keys())
missing_percentages = list(missing_data.values())

# Create a color map that groups colors by the key column.
key_column_colors = {
    'domain': 'blue',
    'name': 'orange',
    'phone': 'purple',
    'category': 'green',
    'city': 'red',
    'country': 'yellow',
    'region': 'pink'
}
bar_colors = []
for cat in categories:
    for key_column in key_column_colors:
        if key_column in cat:
          bar_colors.append(key_column_colors[key_column])
          break
    else:
      bar_colors.append('gray') #Default color if no key column is found.

# Create the bar chart.
plt.figure(figsize=(12, 6))
plt.bar(categories, missing_percentages, color=bar_colors) # Colors added
plt.xlabel("Dataset and Column")
plt.ylabel("Missing Value Percentage")
plt.title("Missing Value Percentage for Key Columns across Datasets")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

# Create custom legend for key columns
key_patches = [plt.Rectangle((0, 0), 1, 1, color=key_column_colors[key]) for key in key_column_colors]
plt.legend(key_patches, key_column_colors.keys(), title="Key Columns", loc='upper right')

# Show the plot.
plt.show()

# Analyze the frequency of business categories in each dataset.
def analyze_category_frequency(df):
    """
    Analyzes the frequency of business categories in a DataFrame.

    Args:
      df: The input DataFrame containing a 'category' column.

    Returns:
      A pandas Series representing the category frequency.
    """
    if 'category' not in df.columns:
        return pd.Series()  # Return empty series if 'category' column is missing

    category_counts = df['category'].explode().value_counts()
    return category_counts

# Get category frequency for each dataset
facebook_category_freq = analyze_category_frequency(facebook_df)
google_category_freq = analyze_category_frequency(google_df)
website_category_freq = analyze_category_frequency(website_df)

# Print or visualize the most frequent categories for each dataset
print("Most Frequent Facebook Categories:\n", facebook_category_freq.head(10))
print("\nMost Frequent Google Categories:\n", google_category_freq.head(10))
print("\nMost Frequent Website Categories:\n", website_category_freq.head(10))

# Compare category representation across datasets (example: top 5)


top_n = 50  # Number of top categories to compare
common_categories = set(facebook_category_freq.head(top_n).index) & set(google_category_freq.head(top_n).index) & set(website_category_freq.head(top_n).index)

print("\nCommon Top", top_n, "Categories across datasets:", len(common_categories))

# Calculating relative frequencies within each dataset to identify overrepresentation.
def relative_freq(series):
  return series / series.sum()

relative_freq_facebook = relative_freq(facebook_category_freq)
relative_freq_google = relative_freq(google_category_freq)
relative_freq_website = relative_freq(website_category_freq)

print("\nRelative Frequency:")
for category in common_categories:
  print(f"{category}: Facebook({relative_freq_facebook.get(category, 0):.2f}), Google({relative_freq_google.get(category, 0):.2f}), Website({relative_freq_website.get(category, 0):.2f})")

def plot_category_pie_charts(datasets, top_n=10):
  """Plots pie charts representing the relative frequency of top categories in each dataset."""

  for name, data in datasets.items():
      if 'category' in data.columns:
          category_counts = data['category'].explode().value_counts()
          relative_frequencies = category_counts / category_counts.sum()
          top_categories = relative_frequencies.head(top_n)

          plt.figure(figsize=(8, 8))
          plt.pie(top_categories, labels=top_categories.index, autopct='%1.1f%%', startangle=90)
          plt.title(f"Top {top_n} Categories in {name} Dataset")
          plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
          plt.show()
      else:
          print(f"The '{name}' dataset does not contain a 'category' column.")

datasets = {
    "Facebook": facebook_df,
    "Google": google_df,
    "Website": website_df
}
plot_category_pie_charts(datasets)

"""### Conclusion


Analysis of the datasets has brought me to the following conclusions:

- The unique count of google domain values compared against the aproximate volume of the facebook and website datasets is indicative of an approximately equal volume in google as well after deduplication which may lead to better accuracy in merging.

- Through analyzing missing values in each column we can clearly see that the best candidates for merging are name, phone and domain as they have no apparent missing values.

- By analyzing category distribution across the datasets, we can certainly say there is no over-representation of one particular category out of the top 10, meaning the dataset is well balanced and can be used (after further processing) for training on DNN's.


"""

website_linktr_ee_count = website_df[website_df['domain'].str.contains('linktr.ee', na=False)].shape[0]
google_linktr_ee_count = google_df[google_df['domain'].str.contains('linktr.ee', na=False)].shape[0]
facebook_linktr_ee_count = facebook_df[facebook_df['domain'].str.contains('linktr.ee', na=False)].shape[0]
print(f"Number of entries with 'linktr.ee' in domain column:")
print(f"Website: {website_linktr_ee_count}")
print(f"Google: {google_linktr_ee_count}")
print(f"Facebook: {facebook_linktr_ee_count}")

# Separate linktr.ee rows from the datasets
facebook_linktree = facebook_df[facebook_df["domain"] == "linktr.ee"]
google_linktree = google_df[google_df["domain"] == "linktr.ee"]
website_linktree = website_df[website_df["domain"] == "linktr.ee"]

# Keep only regular domains
facebook_df = facebook_df[facebook_df["domain"] != "linktr.ee"]
google_df = google_df[google_df["domain"] != "linktr.ee"]
website_df = website_df[website_df["domain"] != "linktr.ee"]

# Merge linktr.ee rows using 'link' as a key instead of 'domain'
linktree_merged = facebook_linktree.merge(google_linktree, on="phone", how="outer", suffixes=("_facebook", "_google"))
linktree_merged = linktree_merged.merge(website_linktree, on="phone", how="outer")

common_columns_fb_google = set(facebook_df.columns) & set(google_df.columns)
print("Common columns between facebook_df and google_df:", common_columns_fb_google)

# Conflict resolution (prioritize website over google, and google over facebook)
linktree_merged['domain'] = linktree_merged.get('domain_google').fillna(linktree_merged.get('domain_facebook'))
linktree_merged['name'] = linktree_merged.get('name').fillna(linktree_merged.get('name_google')).fillna(linktree_merged.get('name_facebook'))
linktree_merged['phone'] = pd.Series(pd.concat([linktree_merged.get('phone'), linktree_merged.get('phone_google'), linktree_merged.get('phone_facebook'), linktree_merged.get('raw_phone')], axis=1).bfill(axis=1).iloc[:, 0])
linktree_merged['city'] = linktree_merged.get('city').fillna(linktree_merged.get('city_google')).fillna(linktree_merged.get('city_facebook'))
linktree_merged['country'] = linktree_merged.get('country').fillna(linktree_merged.get('country_google')).fillna(linktree_merged.get('country_facebook'))
linktree_merged['region'] = linktree_merged.get('region').fillna(linktree_merged.get('region_google')).fillna(linktree_merged.get('region_facebook'))

def merge_categories(row):
    """Combines the category lists from multiple columns into a single list, removing duplicates."""
    categories = set()
    for col in ['category', 'category_google', 'category_facebook']:
        if isinstance(row.get(col, None), list):
            categories.update(row[col])
    return list(categories) if categories else None

linktree_merged['category'] = linktree_merged.apply(merge_categories, axis=1)

linktree_merged['region_code'] = linktree_merged.get('region_code_google').fillna(linktree_merged.get('region_code_facebook'))
linktree_merged['address'] = linktree_merged.get('raw_address').fillna(linktree_merged.get('address_google')).fillna(linktree_merged.get('address_facebook'))
linktree_merged['country_code'] = linktree_merged.get('country_code_google').fillna(linktree_merged.get('country_code_facebook'))
linktree_merged['phone_country_code'] = linktree_merged.get('phone_country_code_google').fillna(linktree_merged.get('phone_country_code_facebook'))
linktree_merged['description'] = linktree_merged.get('description_google').fillna(linktree_merged.get('description_facebook'))
linktree_merged['zip_code'] = linktree_merged.get('zip_code_google').fillna(linktree_merged.get('zip_code_facebook'))

linktree_merged.drop(columns=[
    'domain_google',
    'domain_facebook',

    'name_google',
    'name_facebook',

    'phone_google',
    'phone_facebook',

    'city_google',
    'city_facebook',

    'country_google',
    'country_facebook',

    'region_google',
    'region_facebook',

    'category_google',
    'category_facebook',

    'region_code_google',
    'region_code_facebook',

    'address_google',
    'address_facebook',

    'country_code_google',
    'country_code_facebook',

    'phone_country_code_google',
    'phone_country_code_facebook',

    'description_google',
    'description_facebook',

    'zip_code_google',
    'zip_code_facebook',

    'country_code',
    'region_code',
    'phone_country_code',
    'domain_suffix',
    'tld',
    'page_type',
    'language',
    'raw_phone',
    'raw_address',

    ], inplace=True, errors='ignore')

print("\nMerged DataFrame after conflict resolution:\n", linktree_merged.info())

def deduplicate_phone(df):
    """Deduplicates the 'phone' column of a DataFrame, keeping the first occurrence."""
    return df.drop_duplicates(subset='phone', keep='first')

linktree_merged = deduplicate_phone(linktree_merged)

print("\nLinktree DataFrame size after deduplication:\n", linktree_merged.shape)

# Deduplication on 'domain' column
def deduplicate_domain(df):
    """Deduplicates the 'domain' column of a DataFrame, keeping the first occurrence."""
    return df.drop_duplicates(subset='domain', keep='first')

google_df = deduplicate_domain(google_df)
facebook_df = deduplicate_domain(facebook_df)
website_df = deduplicate_domain(website_df)

print("\nGoogle DataFrame size after deduplication:\n", google_df.shape)
print("\nFacebook DataFrame size after deduplication:\n", facebook_df.shape)
print("\nWebsite DataFrame size after deduplication:\n", website_df.shape)

# Drop rows with missing 'domain' values
def drop_missing_domain(df):
    """Drops rows where the 'domain' column is missing."""
    return df.dropna(subset=['domain'])

google_df = drop_missing_domain(google_df)
facebook_df = drop_missing_domain(facebook_df)
website_df = drop_missing_domain(website_df)

print("\nGoogle DataFrame size after dropping missing 'domain' values:\n", google_df.shape)
print("\nFacebook DataFrame size after dropping missing 'domain' values:\n", facebook_df.shape)
print("\nWebsite DataFrame size after dropping missing 'domain' values:\n", website_df.shape)

merged_df = facebook_df.merge(google_df, on='domain', how='outer', suffixes=('_facebook', '_google'))
merged_df = merged_df.merge(website_df, on='domain', how='outer')

# Conflict resolution (prioritize website over google, and google over facebook)
merged_df['name'] = merged_df['name'].fillna(merged_df['name_google']).fillna(merged_df['name_facebook'])
merged_df['phone'] = merged_df['phone'].fillna(merged_df['phone_google']).fillna(merged_df['phone_facebook']).fillna(merged_df['raw_phone'])
merged_df['city'] = merged_df['city'].fillna(merged_df['city_google']).fillna(merged_df['city_facebook'])
merged_df['country'] = merged_df['country'].fillna(merged_df['country_google']).fillna(merged_df['country_facebook'])
merged_df['region'] = merged_df['region'].fillna(merged_df['region_google']).fillna(merged_df['region_facebook'])

def merge_categories(row):
    """Combines the category lists from multiple columns into a single list, removing duplicates."""
    categories = set()
    for col in ['category', 'category_google', 'category_facebook']:
        if col in row and isinstance(row[col], list):
            categories.update(row[col])
    return list(categories)

merged_df['category'] = merged_df.apply(merge_categories, axis=1)

merged_df['region_code'] = merged_df['region_code_google'].fillna(merged_df['region_code_facebook'])
merged_df['address'] = merged_df['raw_address'].fillna(merged_df['address_google']).fillna(merged_df['address_facebook'])
merged_df['country_code'] = merged_df['country_code_google'].fillna(merged_df['country_code_facebook'])
merged_df['phone_country_code'] = merged_df['phone_country_code_google'].fillna(merged_df['phone_country_code_facebook'])
merged_df['description'] = merged_df['description_google'].fillna(merged_df['description_facebook'])
merged_df['zip_code'] = merged_df['zip_code_google'].fillna(merged_df['zip_code_facebook'])

merged_df.drop(columns=[
    'name_google',
    'name_facebook',

    'phone_google',
    'phone_facebook',

    'city_google',
    'city_facebook',

    'country_google',
    'country_facebook',

    'region_google',
    'region_facebook',

    'category_google',
    'category_facebook',

    'region_code_google',
    'region_code_facebook',

    'address_google',
    'address_facebook',

    'country_code_google',
    'country_code_facebook',

    'phone_country_code_google',
    'phone_country_code_facebook',

    'description_google',
    'description_facebook',

    'zip_code_google',
    'zip_code_facebook',

    'country_code',
    'region_code',
    'phone_country_code',
    'domain_suffix',
    'tld',
    'page_type',
    'language',
    'raw_phone',
    'raw_address',

    ], inplace=True)

print("\nMerged DataFrame after conflict resolution:\n", merged_df.info())

final_df = pd.concat([merged_df, linktree_merged], ignore_index=True)

print(final_df.info())

final_df[:-300]

def calculate_non_null_percentage(df):
  """Calculates the percentage of non-null values for each column in a DataFrame."""
  non_null_percentages = df.notnull().sum() / len(df) * 100
  return non_null_percentages


coverage = calculate_non_null_percentage(final_df)
print(coverage)